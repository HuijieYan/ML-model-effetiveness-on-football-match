{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "121ac1b0",
   "metadata": {},
   "source": [
    "# Beat the Bookie\n",
    "Group L"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b169634e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "To run code:\n",
    "- Create new Anaconda environment (Python 3.8.1)\n",
    "- Install Jupyter\n",
    "- Install following libraries:\n",
    "    - scikit-learn (sklearn)\n",
    "    - matplotlib\n",
    "    - pandas\n",
    "    - seaborn\n",
    "    - category_encoders\n",
    "    - jinja2\n",
    "    - tqdm\n",
    "    - xgboost\n",
    "    - ipywidgets\n",
    "- May also prompt for ipykernel install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e547378",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo Skipping requirement installation by default, comment this out to install dependencies\n",
    "\n",
<<<<<<< HEAD
    "# import sys\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install numpy\n",
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install sklearn\n",
    "# !{sys.executable} -m pip install seaborn\n",
    "# !{sys.executable} -m pip install category_encoders\n",
    "# !{sys.executable} -m pip install ipython-cache"
=======
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install seaborn\n",
    "!{sys.executable} -m pip install category_encoders\n",
    "!{sys.executable} -m pip install ipython-cache\n",
    "!{sys.executable} -m pip install jinja2\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install xgboost\n",
    "!{sys.executable} -m pip install ipywidgets"
>>>>>>> f56f7973ac88f6888ac532399bd567bab91fe67c
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90c552",
   "metadata": {},
   "source": [
    "## Data import\n",
    "1. Import necessary Python libraries.\n",
    "2. Read data files.\n",
    "3. Separate into train/validate/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import datetime\n",
    "from sklearn import naive_bayes, neighbors, ensemble, feature_selection\n",
    "from category_encoders import TargetEncoder, HashingEncoder\n",
    "import cache_magic\n",
<<<<<<< HEAD
    "\n",
    "import json\n",
    "import requests\n",
    "import progressbar\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = requests.get('https://www.premierleague.com/stats/top/clubs/wins?se=489')\n",
    "soup = BeautifulSoup(webpage.text, 'html.parser')\n",
    "\n",
    "if not os.path.exists('files'):\n",
    "    os.makedirs('files/stats')\n",
    "    os.makedirs('files/results')\n",
    "\n",
    "\n",
    "def attributes(links):\n",
    "    return [link[link.rfind('/')+1:] for link in links]\n",
    "\n",
    "def uniques(links):\n",
    "    l = []\n",
    "    for link in links:\n",
    "        if link not in l:\n",
    "            l.append(link)\n",
    "    return l\n",
    "\n",
    "top = [link['href'] for link in soup.select('a.topStatsLink')]\n",
    "more = [link['href'] for link in soup.select('nav.moreStatsMenu a')]\n",
    "links = uniques(attributes(more) + attributes(top))\n",
    "\n",
    "\n",
    "dates = {'2000-2001':9, '2001-2002':10, '2002-2003':11, '2003-2004':12,\n",
    "         '2004-2005':13, '2005-2006':14, '2006-2007':15, '2007-2008':16,\n",
    "         '2008-2009':17, '2009-2010':18, '2010-2011':19, '2011-2012':20,\n",
    "         '2012-2013':21, '2013-2014':22, '2014-2015':27, '2015-2016':42,\n",
    "         '2016-2017':54, '2017-2018':79, '2018-2019':210, '2019-2020':274,\n",
    "         '2020-2021':363, '2021-2022':418, '2022-2023':489}\n",
    "\n",
    "\n",
    "def crawl_club_data():\n",
    "    for date in dates.keys():\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        bar = progressbar.ProgressBar(maxval=len(links), widgets=[date + '\\t', progressbar.Bar('-', '[', ']'), ' ', progressbar.Percentage()])\n",
    "        bar.start()\n",
    "        for i, attribute in zip(range(len(links)), links):\n",
    "\n",
    "            # setup\n",
    "            api = 'https://footballapi.pulselive.com/football/stats/ranked/teams/' + attribute\n",
    "            headers = {'Origin': 'https://www.premierleague.com'}\n",
    "            params = {'page': '0', 'pageSize': '20', 'compSeasons': dates[date], 'comps': '1', 'altIds': 'true'}\n",
    "\n",
    "            # request\n",
    "            response = requests.get(api, params=params, headers=headers)\n",
    "            data = json.loads(response.text)\n",
    "\n",
    "            # parse\n",
    "            teams = []; values = [];\n",
    "            for team in data['stats']['content']:\n",
    "                teams.append(team['owner']['name'])\n",
    "                values.append(team['value'])\n",
    "            series = pd.Series(values, teams, float, attribute)\n",
    "            if df.index.empty:\n",
    "                df = pd.DataFrame(series)\n",
    "            else:\n",
    "                df = df.join(series)\n",
    "\n",
    "            # progress\n",
    "            bar.update(i+1)\n",
    "\n",
    "        bar.finish()\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "        df.to_csv('files/stats/' + date + '.csv')\n",
    "\n",
    "\n",
    "# comment this if you already download\n",
    "crawl_club_data()"
=======
    "from copy import deepcopy\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm as progressbar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bc8aabb",
   "metadata": {},
   "source": [
    "### Averages utility functions\n",
    "Next cells contain functions for getting averages of columns, getting last 5 games, and moving averages"
>>>>>>> f56f7973ac88f6888ac532399bd567bab91fe67c
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b60135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_column(homeTarget, awayTarget):\n",
    "    data_folder = 'Data_Files/'\n",
    "    basic_dataset_filename = 'epl-training.csv'\n",
    "    basic_dataset_path = os.path.join(os.getcwd(), data_folder, 'epl-training.csv')\n",
    "\n",
    "    # removing all non pre-game features\n",
    "    dataset = pd.read_csv(basic_dataset_path)[[\"HomeTeam\", \"AwayTeam\", homeTarget, awayTarget]]\n",
    "\n",
    "    # remove nan values\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    average_dict_home = {}\n",
    "    average_dict_away = {}\n",
    "    for index, record in dataset.iterrows():\n",
    "        if record[\"HomeTeam\"] in average_dict_home:\n",
    "            average_dict_home[record[\"HomeTeam\"]] += record[homeTarget]\n",
    "        else:\n",
    "            average_dict_home[record[\"HomeTeam\"]] = record[homeTarget]\n",
    "        if record[\"AwayTeam\"] in average_dict_away:\n",
    "            average_dict_away[record[\"AwayTeam\"]] += record[awayTarget]\n",
    "        else:\n",
    "            average_dict_away[record[\"AwayTeam\"]] = record[awayTarget]\n",
    "\n",
    "    for team in average_dict_home:\n",
    "        average_dict_home[team] /= len(dataset[dataset[\"HomeTeam\"] == team])\n",
    "    for team in average_dict_away:\n",
    "        average_dict_away[team] /= len(dataset[dataset[\"AwayTeam\"] == team])\n",
    "\n",
    "    return average_dict_home, average_dict_away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_won_last_5_matches(team, date, dataset):\n",
    "    last_5_matches = dataset[(dataset[\"Date\"] < date) & ((dataset[\"HomeTeam\"] == team) | (dataset[\"AwayTeam\"] == team))].tail(5)\n",
    "    won = 0\n",
    "    for index, record in last_5_matches.iterrows():\n",
    "        if record[\"HomeTeam\"] == team:\n",
    "            if record[\"FTR\"] == \"H\":\n",
    "                won += 1\n",
    "        else:\n",
    "            if record[\"FTR\"] == \"A\":\n",
    "                won += 1\n",
    "    if won == 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e508958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(team, date, home, dataset, target, window_size):\n",
    "    if home:\n",
    "        last_matches = dataset[(dataset[\"Date\"] < date) & ((dataset[\"HomeTeam\"] == team))].tail(window_size)\n",
    "    else:\n",
    "        last_matches = dataset[(dataset[\"Date\"] < date) & ((dataset[\"AwayTeam\"] == team))].tail(window_size)\n",
    "    sum = 0\n",
    "    for index, record in last_matches.iterrows():\n",
    "        sum += record[target]\n",
    "\n",
    "    window_size = 1 if len(last_matches) == 0 else len(last_matches)\n",
    "    \n",
    "    return sum / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657d56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('basic', <function load_basic_dataset at 0x0000025A715233A0>)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_folder = 'Data_Files/'\n",
    "\n",
    "# --- loads basic training data provided from course ---\n",
    "\n",
    "# Referee HF AF HY AY HR AR\n",
    "# FTHG FTAG HTHG HTAG\n",
    "# HS AS HST AST HC AC\n",
    "pre_game_features_basic = [\"Date\", \"HomeTeam\", \"AwayTeam\", \"HS\", \"AS\", \"HST\", \"AST\", \"HC\", \"AC\"]\n",
    "target_feature = [\"FTR\"]\n",
    "\n",
    "def load_basic_dataset():\n",
    "    basic_dataset_filename = 'epl-training.csv'\n",
    "    basic_dataset_path = os.path.join(os.getcwd(), data_folder, 'epl-training.csv')\n",
    "\n",
    "    # removing all non pre-game features\n",
    "    dataset = pd.read_csv(basic_dataset_path)[pre_game_features_basic + target_feature]\n",
    "\n",
    "    # remove nan values\n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def short_basic_dataset():\n",
    "    sdf = load_basic_dataset()\n",
    "    \n",
    "    return sdf.tail(int(sdf.shape[0]*0.1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_large_dataset():\n",
    "    large_dataset_filename = 'large_dataset.csv'\n",
    "    large_dataset_path = os.path.join(os.getcwd(), data_folder, large_dataset_filename)\n",
    "    if os.path.exists(large_dataset_path):\n",
    "        return pd.read_csv(large_dataset_path)\n",
    "\n",
    "    pre_game_features_average = [\"Date\", \"HomeTeam\", \"AwayTeam\", \"Referee\", \"FTHG\", \"FTAG\", \"HS\", \"AS\", \"HST\", \"AST\", \"HC\", \"AC\", \"HF\",\"AF\", \"HY\", \"AY\", \"HR\", \"AR\"]\n",
    "    target_feature = [\"FTR\"]\n",
    "    dataset_path = os.path.join(os.getcwd(), data_folder, 'epl-training.csv')\n",
    "    dataset = pd.read_csv(dataset_path)[pre_game_features_average + target_feature]\n",
    "\n",
    "    # goals moving average\n",
    "    dataset[\"HGMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"FTHG\", 20), axis=1)\n",
    "    dataset[\"AGMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"FTAG\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"FTHG\", \"FTAG\"])\n",
    "\n",
    "    # shots moving average\n",
    "    dataset[\"HSMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HS\", 20), axis=1)\n",
    "    dataset[\"ASMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AS\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HS\", \"AS\"])\n",
    "\n",
    "    # shots on target moving average\n",
    "    dataset[\"HSTMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HST\", 20), axis=1)\n",
    "    dataset[\"ASTMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AST\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HST\", \"AST\"])\n",
    "\n",
    "    # corners moving average\n",
    "    dataset[\"HCMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HC\", 20), axis=1)\n",
    "    dataset[\"ACMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AC\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HC\", \"AC\"])\n",
    "\n",
    "    # free kicks moving average\n",
    "    dataset[\"HFMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HF\", 20), axis=1)\n",
    "    dataset[\"AFMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AF\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HF\", \"AF\"])\n",
    "\n",
    "    # yellow cards moving average\n",
    "    dataset[\"HYMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HY\", 20), axis=1)\n",
    "    dataset[\"AYMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AY\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HY\", \"AY\"])\n",
    "\n",
    "    # red cards moving average\n",
    "    dataset[\"HRMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HR\", 20), axis=1)\n",
    "    dataset[\"ARMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AR\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HR\", \"AR\"])\n",
    "\n",
    "\n",
    "    # dataset[\"HWinLast5\"] = dataset.apply(lambda row: team_won_last_5_matches(row[\"HomeTeam\"], row[\"Date\"], True, dataset), axis=1)\n",
    "    # dataset[\"AWinLast5\"] = dataset.apply(lambda row: team_won_last_5_matches(row[\"AwayTeam\"], row[\"Date\"], False, dataset), axis=1)\n",
    "\n",
    "    # normalise moving average columns using standard scaler\n",
    "    scaler = sk.preprocessing.StandardScaler()\n",
    "    dataset[[\"HGMA\", \"AGMA\", \"HSMA\", \"ASMA\", \"HSTMA\", \"ASTMA\", \"HCMA\", \"ACMA\", \"HFMA\", \"AFMA\", \"HYMA\", \"AYMA\", \"HRMA\", \"ARMA\"]] = scaler.fit_transform(dataset[[\"HGMA\", \"AGMA\", \"HSMA\", \"ASMA\", \"HSTMA\", \"ASTMA\", \"HCMA\", \"ACMA\", \"HFMA\", \"AFMA\", \"HYMA\", \"AYMA\", \"HRMA\", \"ARMA\"]])\n",
    "\n",
    "    # write dataset to csv\n",
    "    dataset.to_csv(\"Data_Files/large_dataset.csv\", index=False)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# --- all datasets ---\n",
    "\n",
<<<<<<< HEAD
    "datasets = [(\"basic\", load_basic_dataset)]\n",
    "print(datasets)"
=======
    "def load_dataset_6():\n",
    "    pre_game_features_average = [\"Date\", \"HomeTeam\", \"AwayTeam\", \"FTHG\", \"FTAG\"]\n",
    "    target_feature = [\"FTR\"]\n",
    "    dataset_path = os.path.join(os.getcwd(), data_folder, 'epl-training.csv')\n",
    "\n",
    "    goals_average_home_dict, goals_average_away_dict = get_average_column(\"FTHG\", \"FTAG\")\n",
    "    \n",
    "    dataset = pd.read_csv(dataset_path)[pre_game_features_average + target_feature]\n",
    "    dataset[\"HAG\"] = dataset[\"HomeTeam\"].map(goals_average_home_dict)\n",
    "    dataset[\"AAG\"] = dataset[\"AwayTeam\"].map(goals_average_away_dict)\n",
    "    dataset = dataset.drop(columns=[\"FTHG\", \"FTAG\"])\n",
    "    return dataset\n",
    "\n",
    "def load_new_dataset2():\n",
    "    dataset = load_dataset_6()\n",
    "\n",
    "    shots_average_home_dict, shots_average_away_dict = get_average_column(\"HS\", \"AS\")\n",
    "    dataset[\"HAS\"] = dataset[\"HomeTeam\"].map(shots_average_home_dict)\n",
    "    dataset[\"AAS\"] = dataset[\"AwayTeam\"].map(shots_average_away_dict)\n",
    "\n",
    "    corners_average_home_dict, corners_average_away_dict = get_average_column(\"HC\", \"AC\")\n",
    "    dataset[\"HAC\"] = dataset[\"HomeTeam\"].map(corners_average_home_dict)\n",
    "    dataset[\"AAC\"] = dataset[\"AwayTeam\"].map(corners_average_away_dict)\n",
    "\n",
    "    shotsontarget_average_home_dict, shotsontarget_average_away_dict = get_average_column(\"HST\", \"AST\")\n",
    "    dataset[\"HAST\"] = dataset[\"HomeTeam\"].map(shotsontarget_average_home_dict)\n",
    "    dataset[\"AAST\"] = dataset[\"AwayTeam\"].map(shotsontarget_average_away_dict)\n",
    "\n",
    "    homefreekicks_average_home_dict, homefreekicks_average_away_dict = get_average_column(\"HF\", \"AF\")\n",
    "    dataset[\"HAHF\"] = dataset[\"HomeTeam\"].map(homefreekicks_average_home_dict)\n",
    "    dataset[\"AAHF\"] = dataset[\"AwayTeam\"].map(homefreekicks_average_away_dict)\n",
    "    \n",
    "\n",
    "    # add win last 5 matches column\n",
    "    dataset[\"HWinLast5\"] = dataset.apply(lambda row: team_won_last_5_matches(row[\"HomeTeam\"], row[\"Date\"], dataset), axis=1)\n",
    "    dataset[\"AWinLast5\"] = dataset.apply(lambda row: team_won_last_5_matches(row[\"AwayTeam\"], row[\"Date\"], dataset), axis=1)\n",
    "    \n",
    "    # dataset = dataset.drop(columns=[\"HomeTeam\", \"AwayTeam\"])\n",
    "\n",
    "    # standardscalar\n",
    "    dataset[[\"HAG\", \"AAG\", \"HAS\", \"AAS\", \"HAC\", \"AAC\", \"HAST\", \"AAST\", \"HAHF\", \"AAHF\"]] = sk.preprocessing.StandardScaler().fit_transform(dataset[[\"HAG\", \"AAG\", \"HAS\", \"AAS\", \"HAC\", \"AAC\", \"HAST\", \"AAST\", \"HAHF\", \"AAHF\"]])\n",
    "    \n",
    "\n",
    "    return dataset\n",
    "\n",
    "def load_large_dataset():\n",
    "    large_dataset_filename = 'large_dataset.csv'\n",
    "    large_dataset_path = os.path.join(os.getcwd(), data_folder, large_dataset_filename)\n",
    "    if os.path.exists(large_dataset_path):\n",
    "        return pd.read_csv(large_dataset_path)\n",
    "\n",
    "    pre_game_features_average = [\"Date\", \"HomeTeam\", \"AwayTeam\", \"Referee\", \"FTHG\", \"FTAG\", \"HS\", \"AS\", \"HST\", \"AST\", \"HC\", \"AC\", \"HF\",\"AF\", \"HY\", \"AY\", \"HR\", \"AR\"]\n",
    "    target_feature = [\"FTR\"]\n",
    "    dataset_path = os.path.join(os.getcwd(), data_folder, 'epl-training.csv')\n",
    "    dataset = pd.read_csv(dataset_path)[pre_game_features_average + target_feature]\n",
    "\n",
    "    # goals moving average\n",
    "    dataset[\"HGMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"FTHG\", 20), axis=1)\n",
    "    dataset[\"AGMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"FTAG\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"FTHG\", \"FTAG\"])\n",
    "\n",
    "    # shots moving average\n",
    "    dataset[\"HSMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HS\", 20), axis=1)\n",
    "    dataset[\"ASMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AS\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HS\", \"AS\"])\n",
    "\n",
    "    # shots on target moving average\n",
    "    dataset[\"HSTMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HST\", 20), axis=1)\n",
    "    dataset[\"ASTMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AST\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HST\", \"AST\"])\n",
    "\n",
    "    # corners moving average\n",
    "    dataset[\"HCMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HC\", 20), axis=1)\n",
    "    dataset[\"ACMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AC\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HC\", \"AC\"])\n",
    "\n",
    "    # free kicks moving average\n",
    "    dataset[\"HFMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HF\", 20), axis=1)\n",
    "    dataset[\"AFMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AF\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HF\", \"AF\"])\n",
    "\n",
    "    # yellow cards moving average\n",
    "    dataset[\"HYMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HY\", 20), axis=1)\n",
    "    dataset[\"AYMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AY\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HY\", \"AY\"])\n",
    "\n",
    "    # red cards moving average\n",
    "    dataset[\"HRMA\"] = dataset.apply(lambda row: get_moving_average(row[\"HomeTeam\"], row[\"Date\"], True, dataset, \"HR\", 20), axis=1)\n",
    "    dataset[\"ARMA\"] = dataset.apply(lambda row: get_moving_average(row[\"AwayTeam\"], row[\"Date\"], False, dataset, \"AR\", 20), axis=1)\n",
    "    dataset = dataset.drop(columns=[\"HR\", \"AR\"])\n",
    "\n",
    "\n",
    "    # dataset[\"HWinLast5\"] = dataset.apply(lambda row: team_won_last_5_matches(row[\"HomeTeam\"], row[\"Date\"], True, dataset), axis=1)\n",
    "    # dataset[\"AWinLast5\"] = dataset.apply(lambda row: team_won_last_5_matches(row[\"AwayTeam\"], row[\"Date\"], False, dataset), axis=1)\n",
    "\n",
    "    # normalise moving average columns using standard scaler\n",
    "    scaler = sk.preprocessing.StandardScaler()\n",
    "    dataset[[\"HGMA\", \"AGMA\", \"HSMA\", \"ASMA\", \"HSTMA\", \"ASTMA\", \"HCMA\", \"ACMA\", \"HFMA\", \"AFMA\", \"HYMA\", \"AYMA\", \"HRMA\", \"ARMA\"]] = scaler.fit_transform(dataset[[\"HGMA\", \"AGMA\", \"HSMA\", \"ASMA\", \"HSTMA\", \"ASTMA\", \"HCMA\", \"ACMA\", \"HFMA\", \"AFMA\", \"HYMA\", \"AYMA\", \"HRMA\", \"ARMA\"]])\n",
    "\n",
    "    # write dataset to csv\n",
    "    dataset.to_csv(\"Data_Files/large_dataset.csv\", index=False)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# --- all datasets ---\n",
    "\n",
    "datasets = {\"basic\": load_basic_dataset, \"large\": load_large_dataset, \"short\": short_basic_dataset}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551cfec",
   "metadata": {},
   "source": [
    "takes in a dataframe of n and returns a the dataframe copied splited horizontally \n",
    "\n",
    "NOTE: This is not the feature target split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5612cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc utility for loading data from Kaggle - irrelevant? \n",
    "\n",
    "# --- download Kaggle data ---\n",
    "#import kaggle\n",
    "\n",
    "def download_kaggle_dataset(dataset_name):\n",
    "    subfolder_name = dataset_name.split('/')[1] # get only name of dataset, not kaggle username\n",
    "    dataset_folder_name = data_folder + 'kaggle_datasets' + '/' + subfolder_name + '/'\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.dataset_download_files(dataset_name, path=dataset_folder_name, unzip=True)\n",
    "\n",
    "#download_kaggle_dataset('stefanoleone992/fifa-22-complete-player-dataset')\n",
    "#download_kaggle_dataset('hugomathien/soccer')\n",
    "\n",
    "# --- load Kaggle data functions ---\n",
    "def load_multiple_csvs(folder_name):\n",
    "    # get all csv files in folder\n",
    "    csv_files = [f for f in os.listdir(folder_name) if f.endswith('.csv')]\n",
    "\n",
    "    # load all csv files\n",
    "    dataframes = []\n",
    "    for csv_file in csv_files:\n",
    "        dataframes.append(pd.read_csv(folder_name + csv_file))\n",
    "\n",
    "    # merge all csv files\n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_from_sqlite3(database_name, table_name):\n",
    "    conn = sqlite3.connect(database_name)\n",
    "    df = pd.read_sql_query(\"SELECT * from \" + table_name, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def load_team_attributes_db():\n",
    "    return load_from_sqlite3(data_folder + 'kaggle_datasets/soccer/database.sqlite', 'Team_Attributes')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##datasets.append((\"sofifa_players\", load_multiple_csvs(data_folder + 'kaggle_datasets/fifa-22-complete-player-dataset/')))\n",
    "##datasets.append((\"soccer_teams\", load_from_sqlite3(data_folder + 'kaggle_datasets/soccer/database.sqlite', 'Team_Attributes')))\n",
    "\n",
    "\n",
    "# --- append datasets ---\n",
    "\n"
>>>>>>> f56f7973ac88f6888ac532399bd567bab91fe67c
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "596f2717",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "Takes in a dataframe of n and returns a the dataframe copied splited horizontally\n",
    "\n",
    "NOTE: This is not the feature target split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6774fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80 - 20 split random\n",
    "# NOTE: boolean mask does not produce 80-20 exact splits!\n",
    "random_split = lambda dataset: map(lambda d: d.copy(), train_test_split(dataset, test_size=0.2, random_state=40))\n",
    "\n",
    "# 80 - 20 split sequencial ordered\n",
    "sequential_split = lambda dataset: map(lambda d: d.copy(),train_test_split(dataset, test_size=0.2, shuffle=False))\n",
    "\n",
    "# all training \n",
    "predict_nosplit = lambda dataset: map(lambda d: d.copy(), (dataset, pd.DataFrame().reindex_like(dataset)))\n",
    "\n",
    "split_functions = {\"random_split\": random_split, \"sequential_split\": sequential_split, \"predict_nosplit\": predict_nosplit}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ae764",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f2e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingSequence:\n",
    "    def __init__(self, sequence=[]):\n",
    "        self.sequence = sequence\n",
    "        \n",
    "    \n",
    "    def fit_transform(self, train, test):\n",
    "        for p in self.sequence:\n",
    "            p.fit(train)\n",
    "            p.transform(train, test)\n",
    "    \n",
    "    def process(self, xs):\n",
    "        for p in self.sequence:\n",
    "            p.transform(xs)\n",
    "            \n",
    "    def inverse(self, df, col=\"FTR\"):\n",
    "        for i in range(len(self.sequence)-1, -1, -1):\n",
    "            self.sequence[i].getInverse(df, col)\n",
    "    \n",
    "    def getRepr(self):\n",
    "        r = \"\"\n",
    "        for s in self.sequence:\n",
    "            r += f\"{s.getRepr()}-\"\n",
    "        return r\n",
    "\n",
    "class DropEmpty:\n",
    "        \n",
    "    def fit(self, *dfs):\n",
    "        pass # no functionality needed\n",
    "                    \n",
    "    def transform(self, *dfs):\n",
    "        for df in dfs:\n",
    "            df.dropna(inplace=True)\n",
    "    \n",
    "    def getInverse(self, df, col):\n",
    "        pass # no functionality needed\n",
    "    \n",
    "    def getRepr(self):\n",
    "        return \"d\"\n",
    "    \n",
    "class LabelEncoding:\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        self.le = le = sk.preprocessing.LabelEncoder()\n",
    "        \n",
    "    def fit(self, *dfs):\n",
    "        for df in dfs:\n",
    "            for col in df.columns:\n",
    "                if col in self.cols:\n",
    "                    self.le.fit(df[col])\n",
    "                    \n",
    "    def transform(self, *dfs):\n",
    "        for df in dfs:\n",
    "            for col in df.columns:\n",
    "                if col in self.cols:\n",
    "                    df[col] = self.le.fit_transform(df[col])\n",
    "    \n",
    "    def getInverse(self, df, col): # required to reverses FTR\n",
    "        if col in self.cols:\n",
    "            df[col] = self.le.inverse_transform(df[col])\n",
    "            \n",
    "    def getRepr(self):\n",
    "        return \"le\"\n",
    "            \n",
    "class DateEncoding:\n",
    "    def __init__(self, cols, a):\n",
    "        self.cols = cols\n",
    "        self.origin_date = datetime.datetime(2000,1,1)\n",
    "        self.a = a\n",
    "        \n",
    "    def fit(self, *dfs):\n",
    "        pass # no functionality needed\n",
    "                    \n",
    "    def transform(self, *dfs):\n",
    "        for df in dfs:\n",
    "            for col in df.columns:\n",
    "                if col in self.cols:\n",
    "                    df[col] = df[col].map(self.days_from_origin)\n",
    "    \n",
    "    def getInverse(self, df, col):\n",
    "        pass # not important, skipped for now\n",
    "            \n",
    "    def days_from_origin(self, t):\n",
    "        if len(t) < 10: # for years 200X, they only write the last two digit\n",
    "            return (datetime.datetime.strptime(t, '%d/%m/%y') - self.origin_date).days * self.a\n",
    "        return (datetime.datetime.strptime(t, '%d/%m/%Y') - self.origin_date).days * self.a\n",
    "    \n",
    "    def getRepr(self):\n",
    "        return f\"de{self.a}\"\n",
    "    \n",
    "class TargetEncoding:\n",
    "    def __init__(self, cols, target=\"FTR\"):\n",
    "        self.cols = cols\n",
    "        self.target = target\n",
    "        self.te = TargetEncoder(cols, smoothing=10, min_samples_leaf=20)\n",
    "        \n",
    "    def fit(self, *dfs):\n",
    "        for df in dfs:\n",
    "            self.te.fit(df.drop(columns=[self.target]), df[self.target])\n",
    "                    \n",
    "    def transform(self, *dfs):\n",
    "        for df in dfs:\n",
    "            encoded = self.te.transform(df.drop(columns=[self.target]))\n",
    "            for col in self.cols:\n",
    "                if col in df:\n",
    "                    df[col] = encoded[col]\n",
    "            \n",
    "    \n",
    "    def getInverse(self, df, col):\n",
    "        pass # not important, skipped for now\n",
    "    \n",
    "    def getRepr(self):\n",
    "        return \"te\"\n",
    "\n",
    "class HashEncoding:\n",
    "    def __init__(self, cols, n=8, target=\"FTR\"):\n",
    "        self.cols = cols\n",
    "        self.target = target\n",
    "        self.he = HashingEncoder(cols=cols, n_components=n)\n",
    "        \n",
    "    def fit(self, *dfs):\n",
    "        for df in dfs:\n",
    "            self.he.fit(df.drop(columns=[self.target]), df[self.target])\n",
    "                    \n",
    "    def transform(self, *dfs):\n",
    "        for df in dfs:\n",
    "            encoded = self.he.transform(df.drop(columns=[self.target]))\n",
    "            df.drop(columns=self.cols, inplace=True)\n",
    "            for col in encoded.columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = encoded[col]\n",
    "    \n",
    "    def getInverse(self, df, col):\n",
    "        pass # not important, skipped for now\n",
    "    \n",
    "    def getRepr(self):\n",
    "        return \"he\"\n",
    "\n",
    "def prepSequence(s):\n",
    "    return lambda: ProcessingSequence(s)\n",
    "\n",
    "    \n",
    "\"\"\" preprocessing_selection = [\n",
    "    lambda: ProcessingSequence([DropEmpty(), \n",
    "                         LabelEncoding([\"FTR\"]), \n",
    "                         LabelEncoding([\"HomeTeam\", \"AwayTeam\", \"Referee\"]), \n",
    "                         DateEncoding([\"Date\"], 0.1)\n",
    "                        ]),\n",
    "]  \"\"\"\n",
    "\n",
    "preprocessing_selection = {\n",
    "    \"label_basic\": prepSequence(\n",
    "        [\n",
    "             DropEmpty(), \n",
    "             LabelEncoding([\"FTR\"]), \n",
    "             LabelEncoding([\"HomeTeam\", \"AwayTeam\"]), \n",
    "             DateEncoding([\"Date\"], 0.01)\n",
    "        ]),\n",
    "    \"encode_large\": prepSequence(\n",
    "        [\n",
    "            DropEmpty(),\n",
    "            LabelEncoding([\"FTR\"]),\n",
    "            LabelEncoding([\"HomeTeam\", \"AwayTeam\"]),\n",
    "            LabelEncoding([\"Referee\"]),\n",
    "            DateEncoding([\"Date\"], 0.01)\n",
    "        ]),\n",
    "    \"targetEncode_dateScale.01_basic\": prepSequence(\n",
    "        [\n",
    "             DropEmpty(), \n",
    "             LabelEncoding([\"FTR\"]), \n",
    "             TargetEncoding([\"HomeTeam\", \"AwayTeam\"]), \n",
    "             DateEncoding([\"Date\"], 0.01)\n",
    "        ]),\n",
    "    \"targetEncode_dateScale.05_basic\": prepSequence(\n",
    "        [\n",
    "             DropEmpty(), \n",
    "             LabelEncoding([\"FTR\"]), \n",
    "             TargetEncoding([\"HomeTeam\", \"AwayTeam\"]), \n",
    "             DateEncoding([\"Date\"], 0.05)\n",
    "        ]),\n",
    "    \"targetEncode_dateScale.2_basic\": prepSequence(\n",
    "        [\n",
    "             DropEmpty(), \n",
    "             LabelEncoding([\"FTR\"]), \n",
    "             TargetEncoding([\"HomeTeam\", \"AwayTeam\"]), \n",
    "             DateEncoding([\"Date\"], 0.2)\n",
    "        ]),\n",
    "    \"targetEncode_dateScale1_basic\": prepSequence(\n",
    "        [\n",
    "             DropEmpty(), \n",
    "             LabelEncoding([\"FTR\"]), \n",
    "             TargetEncoding([\"HomeTeam\", \"AwayTeam\"]), \n",
    "             DateEncoding([\"Date\"], 1)\n",
    "        ]),\n",
    "#     (\"hashEncode_dateScale.01_basic\", lambda: ProcessingSequence([DropEmpty(), \n",
    "#                          LabelEncoding([\"FTR\"]), \n",
    "#                          HashEncoding([\"HomeTeam\", \"AwayTeam\"]), \n",
    "#                          DateEncoding([\"Date\"], 0.01)\n",
    "#                         ]))\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f533a5f6",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "- Sklearn has a whole bunch https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection\n",
    "- SciPy has another whole bunch https://docs.scipy.org/doc/scipy/reference/stats.html#correlation-functions\n",
    "- For recursive feature selection and others that needs the estimator, write it in the other feature selection cell below pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea07d5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ebb8cc1",
   "metadata": {},
   "source": [
    "## Data transformation and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d38a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a41a51c1",
   "metadata": {},
   "source": [
    "## Training\n",
    "We have used the following models (1 python cell per type of model):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa3495",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models.append((\"Naive Bayes\", sk.naive_bayes.GaussianNB))\n",
    "\n",
    "# Naive Bayes\n",
    "models[\"Naive_Bayes\"] = sk.naive_bayes.GaussianNB\n",
    "# gnb = sk.naive_bayes.GaussianNB()\n",
    "# gnb.fit(epl_train_small[chosen_labels], epl_train_small['FTR'])\n",
    "\n",
    "# # predict on test set\n",
    "# gnb.predict(epl_test_small[chosen_labels])\n",
    "# # print accuracy\n",
    "# print(gnb.score(epl_test_small[chosen_labels], epl_test_small['FTR']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a9b19",
   "metadata": {},
   "source": [
    "### K-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models.append((\"KNN k=3\", lambda : sk.neighbors.KNeighborsClassifier(n_neighbors = 3)))\n",
    "#models.append((\"KNN k=5\", lambda : sk.neighbors.KNeighborsClassifier(n_neighbors = 5)))\n",
    "\n",
    "# K-nearest neighbors (KNN)\n",
    "models[\"KNN_k=3\"] = lambda : sk.neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "models[\"KNN_k=5\"] = lambda : sk.neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "# knn = sk.neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "# knn.fit(epl_train_small[chosen_labels], epl_train_small['FTR'])\n",
    "\n",
    "# # predict on test set\n",
    "# knn.predict(epl_test_small[chosen_labels])\n",
    "# # print accuracy\n",
    "# print(knn.score(epl_test_small[chosen_labels], epl_test_small['FTR']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ad102",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbcdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest \n",
    "models[\"RF_n=95\"] = lambda : sk.ensemble.RandomForestClassifier(n_estimators=95)\n",
    "models[\"RF_n=100\"] = lambda : sk.ensemble.RandomForestClassifier(n_estimators=100)\n",
    "models[\"RF_n=105\"] = lambda : sk.ensemble.RandomForestClassifier(n_estimators=105)\n",
    "# rf = sk.ensemble.RandomForestClassifier(n_estimators=100)\n",
    "# rf.fit(epl_train_small[chosen_labels], epl_train_small['FTR'])\n",
    "\n",
    "# # predict on test set\n",
    "# rf.predict(epl_test_small[chosen_labels])\n",
    "# # print accuracy\n",
    "# print(rf.score(epl_test_small[chosen_labels], epl_test_small['FTR']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f35ceed7",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.append((\"SVM\", lambda : sk.svm.SVC(gamma='auto', class_weight='balanced', probability=True)))\n",
    "\n",
    "models[\"SVM\"] = lambda : sk.svm.SVC(gamma='auto', class_weight='balanced', probability=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39db8d1d",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8282446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.append((\"LinearSVC\", lambda : sk.svm.LinearSVC(class_weight='balanced', dual=False, max_iter=100000)))\n",
    "models[\"LinearSVC\"] = lambda : sk.svm.LinearSVC(class_weight='balanced', dual=False, max_iter=100000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06d1e80d",
   "metadata": {},
   "source": [
    "### C5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbeb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c5.0 \n",
    "# models.append((\"C5.0\", lambda : sk.tree.DecisionTreeClassifier()))\n",
    "models[\"C5.0\"] = lambda : sk.tree.DecisionTreeClassifier()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35d39eb4",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# models.append((\"XGBoost\", lambda : xgb.XGBClassifier(grow_policy='lossguide')))\n",
    "#models.append((\"XGBoost\", lambda : xgb.XGBClassifier(grow_policy='depthwise')))\n",
    "\n",
    "models[\"XGBoost\"] = lambda : xgb.XGBClassifier(grow_policy='depthwise')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ca9ab43",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression (MLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eeb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models.append((\"MLR\", lambda : sk.linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg')))\n",
    "models[\"MLR\"] = lambda : sk.linear_model.LogisticRegression(class_weight='balanced', solver='newton-cg', max_iter=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f99b3bc5",
   "metadata": {},
   "source": [
    "### Artificial Neural Networks (ANN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb02016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bc7bdf7",
   "metadata": {},
   "source": [
    "# Dataset Pipelines\n",
    "Each dataset will have their own set of processing pipeline to resolve conflicts between different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "DEBUGTOFILE=True\n",
    "\n",
    "def debug_print(*args, **kwargs):\n",
    "    if DEBUG:\n",
    "        if DEBUGTOFILE:\n",
    "            with open(\"pipeline_execution_debugging.txt\", \"a\") as f:\n",
    "                  print(*args, file=f)\n",
    "        else:\n",
    "            print(*args, **kwargs)\n",
    "\n",
    "class Cache:\n",
    "    def __init__(self):\n",
    "        self.cache = dict() \n",
    "        \n",
    "    def get(self, steps): # steps is a tuple with set length of 4\n",
    "        cached_step = 0\n",
    "        for cached_step in range(len(steps)):\n",
    "            # try adding step\n",
    "            substeps = steps[0:cached_step+1]\n",
    "            if substeps not in self.cache:\n",
    "                break\n",
    "        \n",
    "        if cached_step > 0:\n",
    "            debug_print(f\"  > Using {cached_step} steps from cache\")\n",
    "        cached, pre, m = deepcopy(self.cache[steps[0:cached_step]]) if cached_step != 0 else (None, None, None)\n",
    "        return cached, cached_step, pre, m\n",
    "    \n",
    "    def update(self, steps, data, pre=None, m=None):\n",
    "        debug_print(f\"\\tCaching: {steps}\")\n",
    "        if steps not in self.cache:\n",
    "            self.cache[steps] = (deepcopy(data), deepcopy(pre), deepcopy(m))\n",
    "\n",
    "        \n",
    "class Pipeline:\n",
    "    def __init__(self, datasetName, splitName, preprocessingName, modelName, cache, stop=4):\n",
    "        self.datasetName = datasetName\n",
    "        self.splitName = splitName\n",
    "        self.preprocessingName = preprocessingName\n",
    "        self.modelName = modelName\n",
    "        self.cache = cache\n",
    "        self.stop = stop\n",
    "        \n",
    "        self.preprocessor = None\n",
    "        self.model = None\n",
    "        \n",
    "    def get_tuple(self):\n",
    "        return (self.datasetName, self.splitName, self.preprocessingName, self.modelName)\n",
    "    \n",
    "    def get_from_cache(self):\n",
    "        return self.cache.get(self.get_tuple())\n",
    "    \n",
    "    def update_to_cache(self, step_count, data, pre=None, m=None):\n",
    "        self.cache.update(self.get_tuple()[0:step_count+1], data, pre, m)\n",
    "    \n",
    "    def train_and_predict(self, metrics=[sk.metrics.accuracy_score]):\n",
    "        debug_print(f\"Processing {self.get_tuple()}\")\n",
    "        step = 0\n",
    "        data, skip, self.preprocessor, self.model = self.get_from_cache()\n",
    "        step += skip\n",
    "\n",
    "        while True:\n",
    "            if step == 0: # loading dataset\n",
    "                data = datasets[self.datasetName]()\n",
    "                self.update_to_cache(step, data)\n",
    "            elif step == 1: # spliting data\n",
    "                data = list(split_functions[self.splitName](data))\n",
    "                self.update_to_cache(step, data)\n",
    "            elif step == 2: # preprocessing data\n",
    "                self.preprocessor = preprocessing_selection[self.preprocessingName]()\n",
    "                self.preprocessor.fit_transform(data[0], data[1])\n",
    "                self.update_to_cache(step, data, pre=self.preprocessor)\n",
    "            elif step == 3: # evaluating model\n",
    "                self.model = models[self.modelName]()\n",
    "                xtrain, xtest, ytrain, ytest = data[0].drop(columns=[\"FTR\"]), data[1].drop(columns=[\"FTR\"]), data[0][\"FTR\"], data[1][\"FTR\"]\n",
    "                self.model.fit(xtrain, ytrain.values.ravel())\n",
    "                \n",
    "                ypredict = self.model.predict(xtest)\n",
    "                \n",
    "                metric_outputs = [metrics[i](ytest, ypredict) for i in range(len(metrics))]\n",
    "                data = (ypredict, metric_outputs)\n",
    "\n",
    "                self.update_to_cache(step, data, pre=self.preprocessor, m=self.model)\n",
    "                return data\n",
    "            else:\n",
    "                raise \"processing step undefined\"\n",
    "            step += 1  \n",
    "            if step == self.stop:\n",
    "                return data\n",
    "        \n",
    "        \n",
    "\n",
    "def buildPipelines(*args, cache=Cache()):\n",
    "    pipelines = []\n",
    "    for t in itertools.product(*args):\n",
    "        if len(t) != 4:\n",
    "            debug_print(t)\n",
    "            raise Exception(\"buildPipelines requires 4 posistional iterable argument\")\n",
    "        p = Pipeline(t[0], t[1], t[2], t[3], cache)\n",
    "        pipelines.append(p)\n",
    "    \n",
    "    return pipelines\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "basic_pipeline_combination = [\n",
    "    [\"basic\"], \n",
    "    [\"random_split\", \"sequential_split\"], \n",
    "    [\"label_basic\", \"targetEncode_dateScale.01_basic\", \"targetEncode_dateScale.05_basic\", \"targetEncode_dateScale.2_basic\", \"targetEncode_dateScale1_basic\"], \n",
    "    [\"Naive_Bayes\", \"KNN_k=5\", 'RF_n=100', 'LinearSVC', 'XGBoost', \"MLR\", \"C5.0\", \"SVM\"]\n",
    "]\n",
    "\n",
    "short_pipeline_combination = [\n",
    "    [\"short\"], \n",
    "    [\"random_split\", \"sequential_split\"], \n",
    "    [\"label_basic\", \"targetEncode_dateScale.01_basic\", \"targetEncode_dateScale.05_basic\", \"targetEncode_dateScale.2_basic\", \"targetEncode_dateScale1_basic\"], \n",
    "    [\"Naive_Bayes\", \"KNN_k=3\", 'RF_n=100', 'LinearSVC', 'XGBoost', \"MLR\", \"C5.0\", \"SVM\"]\n",
    "]\n",
    "\n",
    "large_pipeline_combination = [\n",
    "    [\"large\"],\n",
    "    [\"random_split\", \"sequential_split\"],\n",
    "    [\"encode_large\"],\n",
    "    [\"Naive_Bayes\", 'RF_n=100', 'LinearSVC', 'XGBoost', 'MLR']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b1d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Datasets: {[e for e in datasets]}\")\n",
    "print(f\"Split: {[e for e in split_functions]}\")\n",
    "print(f\"Preprocessing: {[e for e in preprocessing_selection]}\")\n",
    "print(f\"Models: {[e for e in models]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e994c1f1",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "- sklearn has a whole bunch https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection\n",
    "- SciPy has another whole bunch https://docs.scipy.org/doc/scipy/reference/stats.html#correlation-functions\n",
    "- For recursive feature selection and others that needs the estimator, write it in the other feature selection cell below pipelining \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b109c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_methods = [sk.feature_selection.r_regression, feature_selection.mutual_info_classif]\n",
    "basic_feature_select_pl = Pipeline(\"basic\", \"predict_nosplit\", \"targetEncode_dateScale1_basic\", None, Cache(), stop=3)\n",
    "\n",
    "def column_score(dataset, method):\n",
    "    return method(dataset.drop(columns=[\"FTR\"]), dataset[\"FTR\"])\n",
    "\n",
    "print(column_score(basic_feature_select_pl.train_and_predict()[0], selection_methods[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea7ea50",
   "metadata": {},
   "source": [
    "## Feature Selection 2\n",
    "This section is for feature selections utilizing estimators and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e3bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd3840c",
   "metadata": {},
   "source": [
    "## Validation\n",
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a54649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a53a19",
   "metadata": {},
   "source": [
    "## Results\n",
    "Visualisations and tables to compare models"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "id": "4a82200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on random split basic d-le-le-de0.01- Naive Bayes\n",
      "Working on random split basic d-le-le-de0.01- KNN k=3\n",
      "Working on random split basic d-le-le-de0.01- KNN k=5\n",
      "Working on random split basic d-le-le-de0.01- RF n=95\n",
      "Working on random split basic d-le-le-de0.01- RF n=100\n",
      "Working on random split basic d-le-le-de0.01- RF n=105\n",
      "Working on random split basic d-le-te-de0.01- Naive Bayes\n",
      "Working on random split basic d-le-te-de0.01- KNN k=3\n",
      "Working on random split basic d-le-te-de0.01- KNN k=5\n",
      "Working on random split basic d-le-te-de0.01- RF n=95\n",
      "Working on random split basic d-le-te-de0.01- RF n=100\n",
      "Working on random split basic d-le-te-de0.01- RF n=105\n",
      "Working on random split basic d-le-te-de0.05- Naive Bayes\n",
      "Working on random split basic d-le-te-de0.05- KNN k=3\n",
      "Working on random split basic d-le-te-de0.05- KNN k=5\n",
      "Working on random split basic d-le-te-de0.05- RF n=95\n",
      "Working on random split basic d-le-te-de0.05- RF n=100\n",
      "Working on random split basic d-le-te-de0.05- RF n=105\n",
      "Working on random split basic d-le-te-de0.2- Naive Bayes\n",
      "Working on random split basic d-le-te-de0.2- KNN k=3\n",
      "Working on random split basic d-le-te-de0.2- KNN k=5\n",
      "Working on random split basic d-le-te-de0.2- RF n=95\n",
      "Working on random split basic d-le-te-de0.2- RF n=100\n",
      "Working on random split basic d-le-te-de0.2- RF n=105\n",
      "Working on random split basic d-le-te-de1- Naive Bayes\n",
      "Working on random split basic d-le-te-de1- KNN k=3\n",
      "Working on random split basic d-le-te-de1- KNN k=5\n",
      "Working on random split basic d-le-te-de1- RF n=95\n",
      "Working on random split basic d-le-te-de1- RF n=100\n",
      "Working on random split basic d-le-te-de1- RF n=105\n",
      "Working on sequential split basic d-le-le-de0.01- Naive Bayes\n",
      "Working on sequential split basic d-le-le-de0.01- KNN k=3\n",
      "Working on sequential split basic d-le-le-de0.01- KNN k=5\n",
      "Working on sequential split basic d-le-le-de0.01- RF n=95\n",
      "Working on sequential split basic d-le-le-de0.01- RF n=100\n",
      "Working on sequential split basic d-le-le-de0.01- RF n=105\n",
      "Working on sequential split basic d-le-te-de0.01- Naive Bayes\n",
      "Working on sequential split basic d-le-te-de0.01- KNN k=3\n",
      "Working on sequential split basic d-le-te-de0.01- KNN k=5\n",
      "Working on sequential split basic d-le-te-de0.01- RF n=95\n",
      "Working on sequential split basic d-le-te-de0.01- RF n=100\n",
      "Working on sequential split basic d-le-te-de0.01- RF n=105\n",
      "Working on sequential split basic d-le-te-de0.05- Naive Bayes\n",
      "Working on sequential split basic d-le-te-de0.05- KNN k=3\n",
      "Working on sequential split basic d-le-te-de0.05- KNN k=5\n",
      "Working on sequential split basic d-le-te-de0.05- RF n=95\n",
      "Working on sequential split basic d-le-te-de0.05- RF n=100\n",
      "Working on sequential split basic d-le-te-de0.05- RF n=105\n",
      "Working on sequential split basic d-le-te-de0.2- Naive Bayes\n",
      "Working on sequential split basic d-le-te-de0.2- KNN k=3\n",
      "Working on sequential split basic d-le-te-de0.2- KNN k=5\n",
      "Working on sequential split basic d-le-te-de0.2- RF n=95\n",
      "Working on sequential split basic d-le-te-de0.2- RF n=100\n",
      "Working on sequential split basic d-le-te-de0.2- RF n=105\n",
      "Working on sequential split basic d-le-te-de1- Naive Bayes\n",
      "Working on sequential split basic d-le-te-de1- KNN k=3\n",
      "Working on sequential split basic d-le-te-de1- KNN k=5\n",
      "Working on sequential split basic d-le-te-de1- RF n=95\n",
      "Working on sequential split basic d-le-te-de1- RF n=100\n",
      "Working on sequential split basic d-le-te-de1- RF n=105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1663b_row0_col0, #T_1663b_row3_col0 {\n",
       "  background-color: #ffd300;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row0_col1, #T_1663b_row0_col2 {\n",
       "  background-color: #ff9f00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row0_col3, #T_1663b_row0_col5 {\n",
       "  background-color: #ffe100;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row0_col4 {\n",
       "  background-color: #ffdc00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row1_col0, #T_1663b_row2_col0 {\n",
       "  background-color: #ffd400;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row1_col1 {\n",
       "  background-color: #ff9d00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row1_col2 {\n",
       "  background-color: #ffac00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row1_col3 {\n",
       "  background-color: #ffed00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row1_col4 {\n",
       "  background-color: #ffe700;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row1_col5, #T_1663b_row5_col5, #T_1663b_row9_col4 {\n",
       "  background-color: #ffee00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row2_col1 {\n",
       "  background-color: #ffa200;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row2_col2 {\n",
       "  background-color: #ffa900;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row2_col3, #T_1663b_row2_col5, #T_1663b_row5_col3 {\n",
       "  background-color: #ffea00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row2_col4, #T_1663b_row4_col5 {\n",
       "  background-color: #ffe900;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row3_col1 {\n",
       "  background-color: #ff9b00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row3_col2 {\n",
       "  background-color: #ffa600;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row3_col3 {\n",
       "  background-color: #ffe800;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row3_col4 {\n",
       "  background-color: #ffeb00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row3_col5, #T_1663b_row4_col3 {\n",
       "  background-color: #ffe400;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row4_col0 {\n",
       "  background-color: #ffd600;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row4_col1 {\n",
       "  background-color: #ff8c00;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1663b_row4_col2 {\n",
       "  background-color: #ff8b00;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1663b_row4_col4 {\n",
       "  background-color: #ffec00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row5_col0, #T_1663b_row8_col0, #T_1663b_row9_col0 {\n",
       "  background-color: #ffe300;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row5_col1 {\n",
       "  background-color: #ffab00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row5_col2 {\n",
       "  background-color: #ffb600;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row5_col4 {\n",
       "  background-color: #ffef00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row6_col0, #T_1663b_row7_col0 {\n",
       "  background-color: #ffe200;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row6_col1 {\n",
       "  background-color: #ffc300;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row6_col2 {\n",
       "  background-color: #ffce00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row6_col3, #T_1663b_row9_col5 {\n",
       "  background-color: #fff700;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row6_col4 {\n",
       "  background-color: #fff400;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row6_col5 {\n",
       "  background-color: #fffb00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row7_col1 {\n",
       "  background-color: #ffaf00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row7_col2 {\n",
       "  background-color: #ff9200;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row7_col3 {\n",
       "  background-color: #fff900;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row7_col4, #T_1663b_row9_col3 {\n",
       "  background-color: #fff600;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row7_col5 {\n",
       "  background-color: #fffa00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row8_col1 {\n",
       "  background-color: #ff9700;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row8_col2 {\n",
       "  background-color: #ff9500;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row8_col3 {\n",
       "  background-color: #fffd00;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row8_col4 {\n",
       "  background-color: #fff200;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row8_col5 {\n",
       "  background-color: #fff500;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row9_col1 {\n",
       "  background-color: #ff9900;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1663b_row9_col2 {\n",
       "  background-color: #ff9800;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1663b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"index_name level0\" >Models</th>\n",
       "      <th id=\"T_1663b_level0_col0\" class=\"col_heading level0 col0\" >Naive Bayes</th>\n",
       "      <th id=\"T_1663b_level0_col1\" class=\"col_heading level0 col1\" >KNN k=3</th>\n",
       "      <th id=\"T_1663b_level0_col2\" class=\"col_heading level0 col2\" >KNN k=5</th>\n",
       "      <th id=\"T_1663b_level0_col3\" class=\"col_heading level0 col3\" >RF n=95</th>\n",
       "      <th id=\"T_1663b_level0_col4\" class=\"col_heading level0 col4\" >RF n=100</th>\n",
       "      <th id=\"T_1663b_level0_col5\" class=\"col_heading level0 col5\" >RF n=105</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Segmentation Method</th>\n",
       "      <th class=\"index_name level1\" >Dataset</th>\n",
       "      <th class=\"index_name level2\" >Preprocessing</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1663b_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"5\">random split</th>\n",
       "      <th id=\"T_1663b_level1_row0\" class=\"row_heading level1 row0\" rowspan=\"5\">basic</th>\n",
       "      <th id=\"T_1663b_level2_row0\" class=\"row_heading level2 row0\" >d-le-le-de0.01-</th>\n",
       "      <td id=\"T_1663b_row0_col0\" class=\"data row0 col0\" >0.529758</td>\n",
       "      <td id=\"T_1663b_row0_col1\" class=\"data row0 col1\" >0.449028</td>\n",
       "      <td id=\"T_1663b_row0_col2\" class=\"data row0 col2\" >0.449617</td>\n",
       "      <td id=\"T_1663b_row0_col3\" class=\"data row0 col3\" >0.552151</td>\n",
       "      <td id=\"T_1663b_row0_col4\" class=\"data row0 col4\" >0.545080</td>\n",
       "      <td id=\"T_1663b_row0_col5\" class=\"data row0 col5\" >0.552151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1663b_level2_row1\" class=\"row_heading level2 row1\" >d-le-te-de0.01-</th>\n",
       "      <td id=\"T_1663b_row1_col0\" class=\"data row1 col0\" >0.531526</td>\n",
       "      <td id=\"T_1663b_row1_col1\" class=\"data row1 col1\" >0.446671</td>\n",
       "      <td id=\"T_1663b_row1_col2\" class=\"data row1 col2\" >0.469063</td>\n",
       "      <td id=\"T_1663b_row1_col3\" class=\"data row1 col3\" >0.571597</td>\n",
       "      <td id=\"T_1663b_row1_col4\" class=\"data row1 col4\" >0.561579</td>\n",
       "      <td id=\"T_1663b_row1_col5\" class=\"data row1 col5\" >0.572775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1663b_level2_row2\" class=\"row_heading level2 row2\" >d-le-te-de0.05-</th>\n",
       "      <td id=\"T_1663b_row2_col0\" class=\"data row2 col0\" >0.532115</td>\n",
       "      <td id=\"T_1663b_row2_col1\" class=\"data row2 col1\" >0.454331</td>\n",
       "      <td id=\"T_1663b_row2_col2\" class=\"data row2 col2\" >0.464349</td>\n",
       "      <td id=\"T_1663b_row2_col3\" class=\"data row2 col3\" >0.566293</td>\n",
       "      <td id=\"T_1663b_row2_col4\" class=\"data row2 col4\" >0.564526</td>\n",
       "      <td id=\"T_1663b_row2_col5\" class=\"data row2 col5\" >0.565704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1663b_level2_row3\" class=\"row_heading level2 row3\" >d-le-te-de0.2-</th>\n",
       "      <td id=\"T_1663b_row3_col0\" class=\"data row3 col0\" >0.530937</td>\n",
       "      <td id=\"T_1663b_row3_col1\" class=\"data row3 col1\" >0.443135</td>\n",
       "      <td id=\"T_1663b_row3_col2\" class=\"data row3 col2\" >0.459635</td>\n",
       "      <td id=\"T_1663b_row3_col3\" class=\"data row3 col3\" >0.563347</td>\n",
       "      <td id=\"T_1663b_row3_col4\" class=\"data row3 col4\" >0.567472</td>\n",
       "      <td id=\"T_1663b_row3_col5\" class=\"data row3 col5\" >0.557454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1663b_level2_row4\" class=\"row_heading level2 row4\" >d-le-te-de1-</th>\n",
       "      <td id=\"T_1663b_row4_col0\" class=\"data row4 col0\" >0.535062</td>\n",
       "      <td id=\"T_1663b_row4_col1\" class=\"data row4 col1\" >0.418975</td>\n",
       "      <td id=\"T_1663b_row4_col2\" class=\"data row4 col2\" >0.418385</td>\n",
       "      <td id=\"T_1663b_row4_col3\" class=\"data row4 col3\" >0.556865</td>\n",
       "      <td id=\"T_1663b_row4_col4\" class=\"data row4 col4\" >0.569240</td>\n",
       "      <td id=\"T_1663b_row4_col5\" class=\"data row4 col5\" >0.564526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1663b_level0_row5\" class=\"row_heading level0 row5\" rowspan=\"5\">sequential split</th>\n",
       "      <th id=\"T_1663b_level1_row5\" class=\"row_heading level1 row5\" rowspan=\"5\">basic</th>\n",
       "      <th id=\"T_1663b_level2_row5\" class=\"row_heading level2 row5\" >d-le-le-de0.01-</th>\n",
       "      <td id=\"T_1663b_row5_col0\" class=\"data row5 col0\" >0.554770</td>\n",
       "      <td id=\"T_1663b_row5_col1\" class=\"data row5 col1\" >0.468198</td>\n",
       "      <td id=\"T_1663b_row5_col2\" class=\"data row5 col2\" >0.485866</td>\n",
       "      <td id=\"T_1663b_row5_col3\" class=\"data row5 col3\" >0.566549</td>\n",
       "      <td id=\"T_1663b_row5_col4\" class=\"data row5 col4\" >0.573616</td>\n",
       "      <td id=\"T_1663b_row5_col5\" class=\"data row5 col5\" >0.572438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1663b_level2_row6\" class=\"row_heading level2 row6\" >d-le-te-de0.01-</th>\n",
       "      <td id=\"T_1663b_row6_col0\" class=\"data row6 col0\" >0.554181</td>\n",
       "      <td id=\"T_1663b_row6_col1\" class=\"data row6 col1\" >0.505889</td>\n",
       "      <td id=\"T_1663b_row6_col2\" class=\"data row6 col2\" >0.522968</td>\n",
       "      <td id=\"T_1663b_row6_col3\" class=\"data row6 col3\" >0.586572</td>\n",
       "      <td id=\"T_1663b_row6_col4\" class=\"data row6 col4\" >0.581861</td>\n",
       "      <td id=\"T_1663b_row6_col5\" class=\"data row6 col5\" >0.593640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1663b_level2_row7\" class=\"row_heading level2 row7\" >d-le-te-de0.05-</th>\n",
       "      <td id=\"T_1663b_row7_col0\" class=\"data row7 col0\" >0.554181</td>\n",
       "      <td id=\"T_1663b_row7_col1\" class=\"data row7 col1\" >0.473498</td>\n",
       "      <td id=\"T_1663b_row7_col2\" class=\"data row7 col2\" >0.429329</td>\n",
       "      <td id=\"T_1663b_row7_col3\" class=\"data row7 col3\" >0.590106</td>\n",
       "      <td id=\"T_1663b_row7_col4\" class=\"data row7 col4\" >0.585395</td>\n",
       "      <td id=\"T_1663b_row7_col5\" class=\"data row7 col5\" >0.590695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1663b_level2_row8\" class=\"row_heading level2 row8\" >d-le-te-de0.2-</th>\n",
       "      <td id=\"T_1663b_row8_col0\" class=\"data row8 col0\" >0.554770</td>\n",
       "      <td id=\"T_1663b_row8_col1\" class=\"data row8 col1\" >0.436985</td>\n",
       "      <td id=\"T_1663b_row8_col2\" class=\"data row8 col2\" >0.434040</td>\n",
       "      <td id=\"T_1663b_row8_col3\" class=\"data row8 col3\" >0.595995</td>\n",
       "      <td id=\"T_1663b_row8_col4\" class=\"data row8 col4\" >0.578916</td>\n",
       "      <td id=\"T_1663b_row8_col5\" class=\"data row8 col5\" >0.583628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1663b_level2_row9\" class=\"row_heading level2 row9\" >d-le-te-de1-</th>\n",
       "      <td id=\"T_1663b_row9_col0\" class=\"data row9 col0\" >0.554770</td>\n",
       "      <td id=\"T_1663b_row9_col1\" class=\"data row9 col1\" >0.440518</td>\n",
       "      <td id=\"T_1663b_row9_col2\" class=\"data row9 col2\" >0.438751</td>\n",
       "      <td id=\"T_1663b_row9_col3\" class=\"data row9 col3\" >0.584806</td>\n",
       "      <td id=\"T_1663b_row9_col4\" class=\"data row9 col4\" >0.573027</td>\n",
       "      <td id=\"T_1663b_row9_col5\" class=\"data row9 col5\" >0.585984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25a2f7c11f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def segmented_data_evalution(datasets, models, preprocessing_selection, split_functions):\n",
=======
   "execution_count": null,
   "id": "bdd3c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmented_data_evalution(pipeline_combination):\n",
>>>>>>> f56f7973ac88f6888ac532399bd567bab91fe67c
    "    # creating dataframe\n",
    "    pipelines = buildPipelines(*pipeline_combination)\n",
    "    \n",
    "    index = pd.MultiIndex.from_product(pipeline_combination[:-1], names=[\"Dataset\", \"Segmentation Method\", \"Preprocessing\"])\n",
    "    \n",
    "    table = pd.DataFrame([[None]*len(pipeline_combination[-1]) for i in range(len(pipeline_combination[0])*len(pipeline_combination[1])*len(pipeline_combination[2]))],\n",
    "                      index=index,\n",
    "                      columns=pd.Index(pipeline_combination[-1], name = 'Models'))\n",
    "    \n",
    "    # fitting and scoring feature sets and models\n",
    "    matrix_list = []\n",
    "    for p in progressbar(pipelines):\n",
    "        yTestPredicted, metrics = p.train_and_predict([sk.metrics.accuracy_score, sk.metrics.confusion_matrix])\n",
    "        score, confusion_matrix = metrics[0], metrics[1]\n",
    "\n",
    "        matrix_display = sk.metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=p.model.classes_)\n",
    "        matrix_list.append(matrix_display)\n",
    "        \n",
    "        table.loc[p.get_tuple()[:-1], p.get_tuple()[-1]] = score\n",
    "    \n",
    "    # plotting confusion matrix\n",
    "    index = [0,0]\n",
    "    fig, ax = plt.subplots(len(matrix_list)//5+1, 5, figsize=(20,15))\n",
    "    for matrix in matrix_list:\n",
    "        matrix.plot(ax=ax[index[0]][index[1]], cmap=plt.cm.Blues)\n",
    "        ax[index[0]][index[1]].set_title(f\"{pipelines[matrix_list.index(matrix)].splitName, pipelines[matrix_list.index(matrix)].modelName}\")\n",
    "        index[1] += 1\n",
    "        if index[1] == 5:\n",
    "            index[0] += 1\n",
    "            index[1] = 0\n",
    "        \n",
    "    return table\n",
    "\n",
    "result = segmented_data_evalution(short_pipeline_combination)\n",
    "result.apply(pd.to_numeric).style.background_gradient(axis=None, cmap='autumn', vmin=0.2, vmax = 0.6)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dafd1d",
   "metadata": {},
   "source": [
    "## Final predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05daed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179b316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c458c51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3.8.6 64-bit",
=======
   "display_name": "ml_neural_2",
>>>>>>> f56f7973ac88f6888ac532399bd567bab91fe67c
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea4fe9d80ece6dc73c94dd44d90b57cd8dabe7eb644381e8b91de1486f52c19d"
=======
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "71f0ba4461001ba5096f37a93da37c9d80e0f295815fc78f8f65f836f53e088d"
>>>>>>> f56f7973ac88f6888ac532399bd567bab91fe67c
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
